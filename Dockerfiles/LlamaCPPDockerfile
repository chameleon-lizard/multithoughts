# --- Stage 1: Build the base image ---
FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 AS builder

# Set noninteractive mode to avoid prompts during install
ENV DEBIAN_FRONTEND=noninteractive

# Install necessary dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    cmake \
    build-essential \
    ccache \
    wget \
    unzip \
    && rm -rf /var/lib/apt/lists/*

# Clone the llama.cpp repository
WORKDIR /app
RUN git clone https://github.com/ggerganov/llama.cpp

# Build llama.cpp with CUDA support and server example
WORKDIR /app/llama.cpp
RUN mkdir build && cd build && cmake -DGGML_CUDA=ON -DLLAMA_SERVER=ON .. && make -j $(nproc)

# --- Stage 2:  Prepare the llama.cpp server image ---
FROM ubuntu:22.04 AS server

# Install essential tools
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install libcublas and other necessary libraries.  Find the correct version.
RUN apt-get update && apt-get install -y --no-install-recommends \
    libcublas-dev-12-1 \
    libcusparse-dev-12-1 \
    libcurand-dev-12-1 \
    libcuda1-510 \
    && rm -rf /var/lib/apt/lists/*

# Copy the built server binary from the builder stage
COPY --from=builder /app/llama.cpp/build/server /llama.cpp/server

# Copy all .so files from the build directory.  Careful with this, security risks can be introduced
COPY --from=builder /app/llama.cpp/build/*.so /llama.cpp/

# Create directories and set working directory
RUN mkdir -p /models
WORKDIR /models

# Download the draft model (Qwen 1.5B in Q8_0 quantization)
RUN wget -O DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf \
    "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf"

# Download the main model (Qwen 14B in Q6_K quantization)
RUN wget -O DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf \
    "https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf"

# Expose the port on which the llama.cpp server listens
EXPOSE 8080

# Set the default command to run the llama.cpp server
CMD ["/llama.cpp/server", \
     "--model", "/models/DeepSeek-R1-Distill-Qwen-14B-Q6_K.gguf", \
     "--model-draft", "/models/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf", \
     "-cb", \
     "--n-gpu-layers-draft", "1000", \
     "--n-gpu-layers", "1000", \
     "--ctx-size", "128000", \
     "--prio-batch", "0", \
     "--batch-size", "4", \
     "--flash-attn"]

